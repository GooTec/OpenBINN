#!/usr/bin/env python
"""Summarize gene importance across models and explanation methods.

This script trains logistic regression and a fully connected neural network
on the simulation dataset, computes feature attributions for the FCNN using
several explanation methods, aggregates BINN explanations generated by
``analysis/importance_calculation.py``, and produces a CSV/plot comparing the
estimated importances with the known important genes.
"""
from __future__ import annotations

from pathlib import Path
import argparse
import json
import sys

# ensure repository root is on sys.path so ``openbinn`` can be imported when
# executing this script from within the ``analysis`` directory
ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

import numpy as np
import pandas as pd
import torch
from torch import nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.linear_model import LogisticRegression

from openbinn.explainer import Explainer
import openbinn.experiment_utils as utils
from openbinn.binn.data import PnetSimDataSet, ReactomeNetwork, get_layer_maps

METHODS = ["itg", "ig", "gradshap", "deeplift", "shap"]
NO_BASELINE_METHODS = {"itg", "sg", "grad", "gradshap", "lrp", "lime", "control", "feature_ablation"}
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)


def load_dataset(data_dir: Path):
    ds = PnetSimDataSet(root=str(data_dir), num_features=3)
    ds.split_index_by_file(
        train_fp=data_dir / "splits" / "training_set_0.csv",
        valid_fp=data_dir / "splits" / "validation_set.csv",
        test_fp=data_dir / "splits" / "test_set.csv",
    )
    reactome = ReactomeNetwork(
        dict(
            reactome_base_dir="../biological_knowledge/simulation",
            relations_file_name="SimulationPathwaysRelation.txt",
            pathway_names_file_name="SimulationPathways.txt",
            pathway_genes_file_name="SimulationPathways.gmt",
        )
    )
    maps = get_layer_maps(
        genes=list(ds.node_index),
        reactome=reactome,
        n_levels=3,
        direction="root_to_leaf",
        add_unk_genes=False,
    )
    ds.align_with_map(maps[0].index)
    # ``ds.x`` may not be stored contiguously and its leading dimension may not
    # exactly match ``len(ds.y)``. Use ``reshape`` with the tensor's own size to
    # avoid view-related errors and ensure a proper copy when needed.
    x = ds.x.reshape(ds.x.shape[0], -1).detach().cpu().numpy()
    y = ds.y.reshape(-1).numpy()
    return ds, x, y


def train_logistic(x_train, y_train):
    """Train an L1-regularized logistic regression model."""
    model = LogisticRegression(
        penalty="l1", solver="liblinear", max_iter=1000, random_state=SEED
    )
    model.fit(x_train, y_train)
    return model


def train_fnn(
    x_train, y_train, hidden_dim=128, epochs=50, batch_size=32, lr=1e-3
):
    """Train a simple fully connected neural network."""
    torch.manual_seed(SEED)
    input_dim = x_train.shape[1]
    model = nn.Sequential(
        nn.Linear(input_dim, hidden_dim),
        nn.ReLU(),
        nn.Linear(hidden_dim, 1),
    )
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.BCEWithLogitsLoss()
    dataset = TensorDataset(
        torch.from_numpy(x_train).float(), torch.from_numpy(y_train).float()
    )
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    for _ in range(epochs):
        for xb, yb in loader:
            optimizer.zero_grad()
            logits = model(xb).squeeze(-1)
            # ``reshape_as`` avoids assuming ``yb`` is stored contiguously
            loss = criterion(logits, yb.reshape_as(logits))
            loss.backward()
            optimizer.step()
    return model


def summarize_fcnn(model, x, y, config, n_genes: int, n_feat: int):
    """Return aggregated gene importances for the FCNN model."""
    model.eval()
    x_tensor = torch.from_numpy(x).float()
    y_tensor = torch.from_numpy(y).long()
    summary = {}
    for method in METHODS:
        p_conf = utils.fill_param_dict(method, config['explainers'][method], x_tensor)
        p_conf['classification_type'] = 'binary'
        if method not in NO_BASELINE_METHODS:
            p_conf['baseline'] = torch.zeros_like(x_tensor)
        explainer = Explainer(method, model, p_conf)
        imp = explainer.get_explanations(x_tensor, y_tensor)
        imp = imp.abs().sum(0).detach().cpu().numpy()
        summary[method] = imp.reshape(n_genes, n_feat).sum(axis=1)
    return summary


def summarize_binn(exp_dir: Path):
    """Aggregate BINN explanations averaged across layers."""
    summary = {}
    exp_root = exp_dir / "explanations"
    for method in METHODS:
        layer_files = sorted(exp_root.glob(f"PNet_*_{method}_L*_layer*_test.csv"))
        if not layer_files:
            continue
        layer_imps = []
        for fp in layer_files:
            df = pd.read_csv(fp)
            gene_cols = [c for c in df.columns if c not in {'sample_id', 'label', 'prediction'}]
            layer_imps.append(df[gene_cols].abs().sum(0))
        summary[method] = pd.concat(layer_imps, axis=1).mean(axis=1)
    return summary


def main():
    ap = argparse.ArgumentParser(description="Summarize gene importances")
    ap.add_argument("--data-dir", default="./data/prostate", help="Simulation data directory")
    ap.add_argument("--binn-dir", default="./data/prostate", help="Directory with BINN explanations")
    ap.add_argument("--out-dir", default="importance_summary", help="Output directory")
    args = ap.parse_args()

    out_dir = Path(args.out_dir); out_dir.mkdir(parents=True, exist_ok=True)
    ds, x, y = load_dataset(Path(args.data_dir))
    tr_idx = ds.train_idx

    log_model = train_logistic(x[tr_idx], y[tr_idx])
    n_genes = len(ds.node_index)
    n_feat = ds.x.shape[2]
    beta = np.abs(log_model.coef_[0].reshape(n_genes, n_feat)).sum(axis=1)

    fcnn = train_fnn(x[tr_idx], y[tr_idx])
    root = Path(__file__).resolve().parents[1]
    config = utils.load_config(str(root / "configs/experiment_config.json"))
    fcnn_imp = summarize_fcnn(fcnn, x, y, config, n_genes, n_feat)

    binn_imp = summarize_binn(Path(args.binn_dir))

    true_fp = Path(args.data_dir) / "true_genes.csv"
    if true_fp.exists():
        truth = pd.read_csv(true_fp)
        truth = truth.set_index('gene')['important']
    else:
        raise FileNotFoundError("true_genes.csv not found in data directory")

    genes = ds.node_index
    df = pd.DataFrame({'gene': genes, 'true_gene': truth.reindex(genes).fillna(0).astype(int)})
    df['logistic'] = beta
    for method, vals in fcnn_imp.items():
        df[f'fcnn_{method}'] = vals
    for method, series in binn_imp.items():
        df[f'binn_{method}'] = series.reindex(genes).fillna(0).values
    df.to_csv(out_dir / "gene_importance_summary.csv", index=False)

    # simple scatter plot for one example method
    import matplotlib.pyplot as plt
    plt.figure(figsize=(6,4))
    plt.scatter(df['true_gene'], df['logistic'], label='logistic', alpha=0.7)
    plt.scatter(df['true_gene'], df.get('fcnn_deeplift', np.zeros(len(df))), label='fcnn_deeplift', alpha=0.7)
    plt.scatter(df['true_gene'], df.get('binn_deeplift', np.zeros(len(df))), label='binn_deeplift', alpha=0.7)
    plt.xlabel('True important gene')
    plt.ylabel('Estimated importance')
    plt.legend()
    plt.tight_layout()
    plt.savefig(out_dir / 'gene_importance_scatter.png')

if __name__ == "__main__":
    main()
