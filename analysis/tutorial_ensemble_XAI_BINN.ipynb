{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble XAI on BINN\n",
    "This tutorial trains a BINN on a single simulation dataset and compares six explanation methods: **DeepLift**, **IntegratedGradients**, **GradientShap**, **InputÃ—Gradient**, **SmoothGrad**, and **DeepLiftShap**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "cwd = Path.cwd()\n",
    "if (cwd / 'openbinn').exists():\n",
    "    sys.path.insert(0, str(cwd))\n",
    "elif (cwd.parent / 'openbinn').exists():\n",
    "    sys.path.insert(0, str(cwd.parent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/twgoo/.conda/envs/openBINN/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:response in cached_data is being set by 'data/b1.0_g1.0/1/response.csv'\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name              | Type       | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | network           | ModuleList | 111    | train\n",
      "1 | intermediate_outs | ModuleList | 35     | train\n",
      "---------------------------------------------------------\n",
      "146       Trainable params\n",
      "0         Non-trainable params\n",
      "146       Total params\n",
      "0.001     Total estimated model params size (MB)\n",
      "29        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch_geometric.loader import DataLoader as GeoLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from openbinn.binn import PNet\n",
    "from openbinn.binn.util import InMemoryLogger, get_roc\n",
    "from openbinn.binn.data import PnetSimDataSet, PnetSimExpDataSet, ReactomeNetwork, get_layer_maps\n",
    "from openbinn.explainer import Explainer\n",
    "import openbinn.experiment_utils as utils\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model, target_layer):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.print_layer = target_layer\n",
    "        self.target_layer = target_layer\n",
    "    def forward(self, x):\n",
    "        outs = self.model(x)\n",
    "        return outs[self.target_layer - 1]\n",
    "\n",
    "\n",
    "def load_reactome_once():\n",
    "    return ReactomeNetwork(dict(\n",
    "        reactome_base_dir=\"../biological_knowledge/simulation\",\n",
    "        relations_file_name=\"SimulationPathwaysRelation.txt\",\n",
    "        pathway_names_file_name=\"SimulationPathways.txt\",\n",
    "        pathway_genes_file_name=\"SimulationPathways.gmt\",\n",
    "    ))\n",
    "\n",
    "def train_dataset(scen_dir, reactome, best_params=None):\n",
    "    ds = PnetSimDataSet(root=str(scen_dir), num_features=3)\n",
    "    ds.split_index_by_file(\n",
    "        train_fp=scen_dir/'splits'/'training_set_0.csv',\n",
    "        valid_fp=scen_dir/'splits'/'validation_set.csv',\n",
    "        test_fp =scen_dir/'splits'/'test_set.csv',\n",
    "    )\n",
    "    maps = get_layer_maps(genes=list(ds.node_index), reactome=reactome, n_levels=3, direction='root_to_leaf', add_unk_genes=False)\n",
    "    ds.node_index = [g for g in ds.node_index if g in maps[0].index]\n",
    "    lr = 1e-3 if best_params is None else best_params[0]\n",
    "    bs = 16 if best_params is None else int(best_params[1])\n",
    "    tr_loader = GeoLoader(ds, bs, sampler=SubsetRandomSampler(ds.train_idx), num_workers=0)\n",
    "    va_loader = GeoLoader(ds, bs, sampler=SubsetRandomSampler(ds.valid_idx), num_workers=0)\n",
    "    model = PNet(layers=maps, num_genes=maps[0].shape[0], lr=lr)\n",
    "    trainer = pl.Trainer(accelerator='auto', deterministic=True, max_epochs=200,\n",
    "                         callbacks=[EarlyStopping('val_loss', patience=10, mode='min', verbose=False, min_delta=0.01)],\n",
    "                         logger=InMemoryLogger(), enable_progress_bar=False)\n",
    "    trainer.fit(model, tr_loader, va_loader)\n",
    "    (scen_dir/'results'/'optimal').mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(model.state_dict(), scen_dir/'results'/'optimal'/'trained_model.pth')\n",
    "    return model, maps\n",
    "\n",
    "def explain_dataset(scen_dir, reactome, method):\n",
    "    ds = PnetSimExpDataSet(root=str(scen_dir), num_features=1)\n",
    "    ds.split_index_by_file(\n",
    "        train_fp=scen_dir/'splits'/'training_set_0.csv',\n",
    "        valid_fp=scen_dir/'splits'/'validation_set.csv',\n",
    "        test_fp =scen_dir/'splits'/'test_set.csv',\n",
    "    )\n",
    "    maps = get_layer_maps(genes=list(ds.node_index), reactome=reactome, n_levels=3, direction='root_to_leaf', add_unk_genes=False)\n",
    "    ds.node_index = [g for g in ds.node_index if g in maps[0].index]\n",
    "    model = PNet(layers=maps, num_genes=maps[0].shape[0], lr=0.001)\n",
    "    state = torch.load(scen_dir/'results'/'optimal'/'trained_model.pth', map_location='cpu')\n",
    "    model.load_state_dict(state); model.eval()\n",
    "    loader = GeoLoader(ds, batch_size=len(ds.test_idx), sampler=SubsetRandomSampler(ds.test_idx), num_workers=0)\n",
    "    explain_root = scen_dir/'explanations'\n",
    "    explain_root.mkdir(exist_ok=True)\n",
    "    for tgt in range(1, len(maps)+1):\n",
    "        wrap = ModelWrapper(model, tgt)\n",
    "        expl_acc, lab_acc, pred_acc, id_acc = {}, [], [], []\n",
    "        for X, y, ids in loader:\n",
    "            p_conf = {'baseline': torch.zeros_like(X), 'classification_type': 'binary'}\n",
    "            explainer = Explainer(method, wrap, p_conf)\n",
    "            exp_dict = explainer.get_layer_explanations(X, y)\n",
    "            for lname, ten in exp_dict.items():\n",
    "                expl_acc.setdefault(lname, []).append(ten.detach().cpu().numpy())\n",
    "            lab_acc.append(y.cpu().numpy())\n",
    "            pred_acc.append(wrap(X).detach().cpu().numpy())\n",
    "            id_acc.append(ids)\n",
    "        for idx, (lname, arrs) in enumerate(expl_acc.items()):\n",
    "            if idx >= len(maps):\n",
    "                break\n",
    "            arr = np.concatenate(arrs, axis=0)\n",
    "            labels = np.concatenate(lab_acc, axis=0)\n",
    "            preds  = np.concatenate(pred_acc, axis=0)\n",
    "            all_ids= [sid for batch in id_acc for sid in batch]\n",
    "            cur_map = maps[idx]\n",
    "            cols = list(cur_map.index) if cur_map.shape[0]==arr.shape[1] else list(cur_map.columns)\n",
    "            df = pd.DataFrame(arr, columns=cols)\n",
    "            df['label'] = labels\n",
    "            df['prediction'] = preds\n",
    "            df['sample_id'] = all_ids\n",
    "            out_fp = explain_root / f\"PNet_{method}_L{tgt}_layer{idx}_test.csv\"\n",
    "            df.to_csv(out_fp, index=False)\n",
    "    print('Saved raw importances for', method)\n",
    "reactome = load_reactome_once()\n",
    "scenario = Path('./data/b1.0_g1.0/1')  # beta=1.0, gamma=1.0\n",
    "model, maps = train_dataset(scenario, reactome)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:response in cached_data is being set by 'data/b1.0_g1.0/1/response.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running deeplift ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:response in cached_data is being overwritten by 'data/b1.0_g1.0/1/response.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw importances for deeplift\n",
      "Running ig ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:response in cached_data is being overwritten by 'data/b1.0_g1.0/1/response.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw importances for ig\n",
      "Running gradshap ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:response in cached_data is being overwritten by 'data/b1.0_g1.0/1/response.csv'\n",
      "WARNING:root:response in cached_data is being overwritten by 'data/b1.0_g1.0/1/response.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw importances for gradshap\n",
      "Running itg ...\n",
      "Saved raw importances for itg\n",
      "Running shap ...\n",
      "Saved raw importances for shap\n"
     ]
    }
   ],
   "source": [
    "methods = ['deeplift', 'ig', 'gradshap', 'itg', 'shap']\n",
    "for method in methods:\n",
    "    print(f'Running {method} ...')\n",
    "    explain_dataset(scenario, reactome, method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818503e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openBINN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
